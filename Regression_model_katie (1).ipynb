{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file as dataframe \n",
    "# import pyspark modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, Bucketizer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.mllib.regression as reg\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "import functools \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"app\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.cores.max', '2') \\\n",
    "    .config(\"spark.driver.memory\",'4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(\"/home/jovyan/FlightDelay/clean_data_no_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").option(\"inferschema\",\"true\").load(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"AIRLINE\", outputCol=\"AIRLINE_Index\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"AIRLINE_Index\", outputCol=\"AIRLINE_Vec\")\n",
    "encoded = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer2 = StringIndexer(inputCol=\"ORIGIN_AIRPORT\", outputCol=\"ORIGIN_AIRPORT_Index\")\n",
    "model2 = stringIndexer2.fit(encoded)\n",
    "indexed2 = model2.transform(encoded)\n",
    "\n",
    "encoder2 = OneHotEncoder(inputCol=\"ORIGIN_AIRPORT_Index\", outputCol=\"ORIGIN_AIRPORT_Vec\")\n",
    "encoded2 = encoder2.transform(indexed2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------------+-----------------------+\n",
      "|DESTINATION_AIRPORT|DESTINATION_AIRPORT_Index|DESTINATION_AIRPORT_Vec|\n",
      "+-------------------+-------------------------+-----------------------+\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                CLT|                     14.0|       (585,[14],[1.0])|\n",
      "|                ATL|                      0.0|        (585,[0],[1.0])|\n",
      "|                LAX|                      4.0|        (585,[4],[1.0])|\n",
      "|                IAH|                      5.0|        (585,[5],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                FLL|                     21.0|       (585,[21],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                MCO|                     10.0|       (585,[10],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                PHL|                     25.0|       (585,[25],[1.0])|\n",
      "|                ATL|                      0.0|        (585,[0],[1.0])|\n",
      "|                LAS|                      7.0|        (585,[7],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                SEA|                     11.0|       (585,[11],[1.0])|\n",
      "|                ORD|                      1.0|        (585,[1],[1.0])|\n",
      "|                DEN|                      3.0|        (585,[3],[1.0])|\n",
      "+-------------------+-------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer3 = StringIndexer(inputCol=\"DESTINATION_AIRPORT\", outputCol=\"DESTINATION_AIRPORT_Index\")\n",
    "model3 = stringIndexer3.fit(encoded2)\n",
    "indexed3 = model3.transform(encoded2)\n",
    "\n",
    "encoder3 = OneHotEncoder(inputCol=\"DESTINATION_AIRPORT_Index\", outputCol=\"DESTINATION_AIRPORT_Vec\")\n",
    "encoded3 = encoder3.transform(indexed3)\n",
    "encoded3.select('DESTINATION_AIRPORT','DESTINATION_AIRPORT_Index', \"DESTINATION_AIRPORT_Vec\").show()\n",
    "#encoded3.cache()b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols_to_drop = ['AIRLINE_Index', 'AIRLINE', 'ORIGIN_AIRPORT_Index', \n",
    "                    'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT_Index',\n",
    "                    'DESTINATION_AIRPORT', 'FLIGHT_NUMBER']\n",
    "\n",
    "final_encoded = encoded3.drop(*new_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'DISTANCE',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'B_SCHEDULED_ARRIVAL',\n",
       " 'B_ARRIVAL_TIME',\n",
       " 'B_SCHEDULED_DEPARTURE',\n",
       " 'B_DEPARTURE_TIME',\n",
       " 'AIRLINE_Vec',\n",
       " 'ORIGIN_AIRPORT_Vec',\n",
       " 'DESTINATION_AIRPORT_Vec']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_encoded.drop('_c0', 'YEAR', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ARRIVAL_TIME', \n",
    "                              'B_ARRIVAL_TIME', 'ELAPSED_TIME', 'B_ARRIVAL_TIME', \n",
    "                              'B_DEPARTURE_TIME', 'DEPARTURE_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convery final_encoded to rdd \n",
    "# we cannpot scale bucketized or vec columns, so we omit those form the scaling process\n",
    "input_data = final_df.rdd.map(lambda x: (x[8], DenseVector(x[1:8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sqlCtx.createDataFrame(input_data, [\"label\",\"features2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler(inputCol = \"features2\", outputCol = \"features_scaled\")\n",
    "\n",
    "scaler = SS.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data in df2 with our scaler \n",
    "scaled_df = scaler.transform(df2)\n",
    "#join scalable feature with columns  'AIRLINE_Vec', 'ORIGIN_AIRPORT_Vec', \n",
    "#'DESTINATION_AIRPORT_Vec', 'B_SCHEDULED_ARRIVAL', 'B_ARRIVAL_TIME', \n",
    "#'B_SCHEDULED_DEPARTURE','B_DEPARTURE_TIME'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|           features2|     features_scaled|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[1.0,4.0,200.0,12...|[0.11374658158447...|\n",
      "|    0|[1.0,4.0,220.0,22...|[0.11374658158447...|\n",
      "|    0|[1.0,4.0,545.0,12...|[0.11374658158447...|\n",
      "|    1|[1.0,4.0,545.0,66...|[0.11374658158447...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there is no common column between these two dataframes add row_index so that it can be joined\n",
    "scaled_df = scaled_df.withColumn('row_index', F.monotonically_increasing_id())\n",
    "final_df = final_df.withColumn('row_index', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine scaled df and final_df\n",
    "total_df = scaled_df.join(final_df, scaled_df.row_index == final_df.row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111166"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns of already scaled predictors \n",
    "total_df = total_df.drop(*['MONTH', 'DAY', 'DAY_OF_WEEK', 'SCHEDULED_DEPARTURE', 'SCHEDULED_TIME', 'DISTANCE',\n",
    "                           'SCHEDULED_ARRIVAL', 'DIVERTED', \n",
    "                           'CANCELLED2', 'row_index']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'features2',\n",
       " 'features_scaled',\n",
       " 'CANCELLED',\n",
       " 'B_SCHEDULED_ARRIVAL',\n",
       " 'B_SCHEDULED_DEPARTURE',\n",
       " 'AIRLINE_Vec',\n",
       " 'ORIGIN_AIRPORT_Vec',\n",
       " 'DESTINATION_AIRPORT_Vec']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final scaled dataframe for predicting Cancelled flights \n",
    "total_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPlit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled, test_data_scaled = total_df.randomSplit([0.8, 0.2], seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train_data_scaled.where(total_df.label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to filter out large portion of non-canclelled flights in training data to make data more even \n",
    "not_c = train_data_scaled.where(total_df.label == 0).sample(False, .018, 454) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1618"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_c.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled = unionAll([c, not_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'features2',\n",
       " 'features_scaled',\n",
       " 'CANCELLED',\n",
       " 'B_SCHEDULED_ARRIVAL',\n",
       " 'B_SCHEDULED_DEPARTURE',\n",
       " 'AIRLINE_Vec',\n",
       " 'ORIGIN_AIRPORT_Vec',\n",
       " 'DESTINATION_AIRPORT_Vec']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only using ORIGIN_AIRPORT_Vec to predict cancellelation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_base = train_data_scaled.select([\"ORIGIN_AIRPORT_Vec\", \"CANCELLED\"])\n",
    "test_data_base =  test_data_scaled.select([\"ORIGIN_AIRPORT_Vec\", \"CANCELLED\"])\n",
    "lr = LogisticRegression(featuresCol = 'ORIGIN_AIRPORT_Vec', labelCol='CANCELLED', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "linearModel = lr.fit(train_data_base)\n",
    "predicted = linearModel.transform(test_data_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+--------------------+----------+\n",
      "|ORIGIN_AIRPORT_Vec|CANCELLED|       rawPrediction|         probability|prediction|\n",
      "+------------------+---------+--------------------+--------------------+----------+\n",
      "|   (593,[6],[1.0])|        0|[0.09728988844397...|[0.52430330524951...|       0.0|\n",
      "|  (593,[39],[1.0])|        0|[0.09728988844397...|[0.52430330524951...|       0.0|\n",
      "|  (593,[34],[1.0])|        0|[0.09728988844397...|[0.52430330524951...|       0.0|\n",
      "| (593,[271],[1.0])|        0|[0.09728988844397...|[0.52430330524951...|       0.0|\n",
      "+------------------+---------+--------------------+--------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.500000\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
    "AUC = evaluator.evaluate(predicted)\n",
    "print(\"AUC: %f\" % AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy, precision, recall and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.984950\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"CANCELLED\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predicted)\n",
    "print(\"accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.977482\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"CANCELLED\", predictionCol=\"prediction\")\n",
    "f1 = evaluator.evaluate(predicted)\n",
    "print(\"f1: %f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp = df[(df.target_index == 1) & (df.prediction == 1)].count()\n",
    "tp = predicted[(predicted.CANCELLED == 1) & (predicted.prediction == 1.0)].count()\n",
    "tn = predicted[(predicted.CANCELLED == 0) & (predicted.prediction == 0.0)].count()\n",
    "fp = predicted[(predicted.CANCELLED == 0) & (predicted.prediction == 1.0)].count()\n",
    "fn = predicted[(predicted.CANCELLED == 1) & (predicted.prediction == 0.0)].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0\n"
     ]
    }
   ],
   "source": [
    "# precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn) \n",
    "# print(\"Precision = %g\" % (precision))\n",
    "print(\"Recall = %g\" % (recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 0\n",
      "True Negatives: 21990\n",
      "False Positives: 0\n",
      "False Negatives: 336\n",
      "Total 111166\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix \n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"Total\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'features2',\n",
       " 'features_scaled',\n",
       " 'CANCELLED',\n",
       " 'B_SCHEDULED_ARRIVAL',\n",
       " 'B_SCHEDULED_DEPARTURE',\n",
       " 'AIRLINE_Vec',\n",
       " 'ORIGIN_AIRPORT_Vec',\n",
       " 'DESTINATION_AIRPORT_Vec']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables excluding label, feature2 and CANCELLED\n",
    "vars_to_keep = [\n",
    " 'features_scaled',\n",
    " 'B_SCHEDULED_ARRIVAL',\n",
    " 'B_SCHEDULED_DEPARTURE',\n",
    " 'AIRLINE_Vec',\n",
    " 'ORIGIN_AIRPORT_Vec',\n",
    " 'DESTINATION_AIRPORT_Vec'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "                            inputCols=[c for c in vars_to_keep],\n",
    "                            outputCol='features')\n",
    "train = assembler.transform(train_data_scaled).select(\"CANCELLED\", \"features\")\n",
    "test = assembler.transform(test_data_scaled).select(\"CANCELLED\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression \n",
    "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model \n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Use test set to measure the accuracy of our model on new data\n",
    "predicted = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+----------+\n",
      "|CANCELLED|            features|       rawPrediction|         probability|prediction|\n",
      "+---------+--------------------+--------------------+--------------------+----------+\n",
      "|        0|(1200,[0,1,2,3,4,...|[0.14327907934882...|[0.53575861701754...|       0.0|\n",
      "|        0|(1200,[0,1,2,3,4,...|[0.03118929107003...|[0.50779669074429...|       0.0|\n",
      "|        0|(1200,[0,1,2,3,4,...|[0.13841543288390...|[0.53454871640482...|       0.0|\n",
      "+---------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.695375\n"
     ]
    }
   ],
   "source": [
    "AUC = evaluator.evaluate(predicted)\n",
    "print(\"AUC: %f\" % AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy, precision, recall and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.884037\n"
     ]
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"CANCELLED\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predicted)\n",
    "print(\"accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.925144\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"CANCELLED\", predictionCol=\"prediction\")\n",
    "f1 = evaluator.evaluate(predicted)\n",
    "print(\"f1: %f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp = df[(df.target_index == 1) & (df.prediction == 1)].count()\n",
    "tp = predicted[(predicted.CANCELLED == 1) & (predicted.prediction == 1.0)].count()\n",
    "tn = predicted[(predicted.CANCELLED == 0) & (predicted.prediction == 0.0)].count()\n",
    "fp = predicted[(predicted.CANCELLED == 0) & (predicted.prediction == 1.0)].count()\n",
    "fn = predicted[(predicted.CANCELLED == 1) & (predicted.prediction == 0.0)].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 104\n",
      "True Negatives: 19633\n",
      "False Positives: 2357\n",
      "False Negatives: 232\n",
      "Total 111166\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix \n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"Total\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0422592\n",
      "Recall = 0.309524\n"
     ]
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn) \n",
    "print(\"Precision = %g\" % (precision))\n",
    "print(\"Recall = %g\" % (recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
