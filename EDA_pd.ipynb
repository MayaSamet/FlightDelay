{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Column Descriptions! \n",
    "\n",
    "YEAR Year of the Flight Trip \n",
    "\n",
    "MONTH Month of the Flight Trip\n",
    "\n",
    "DAY Day of the Flight Trip \n",
    "\n",
    "DAY_OF_WEEK Day of week of the Flight Trip\n",
    "\n",
    "AIRLINE Airline Identifier\n",
    "\n",
    "FLIGHT_NUMBER Flight Identifier\n",
    "\n",
    "TAIL_NUMBER Aircraft Identifier\n",
    "\n",
    "ORIGIN_AIRPORT Starting Airport\n",
    "\n",
    "DESTINATION_AIRPORT  Destination Airport\n",
    "\n",
    "SCHEDULED_DEPARTURE  Planned Departure Time\n",
    "\n",
    "DEPARTURE_TIME: WHEEL_OFF - TAXI_OUT\n",
    "\n",
    "DEPARTURE_DELAY  Total Delay on Departure\n",
    "\n",
    "TAXI_OUT The time duration elapsed between departure from the origin airport gate and wheels off\n",
    "\n",
    "WHEELS_OFF The time point that the aircraft's wheels leave the ground\n",
    "\n",
    "SCHEDULED_TIME: Planned time amount needed for the flight trip\n",
    "\n",
    "ELAPSED_TIME:  AIR_TIME + TAXI_IN + TAXI_OUT\n",
    "\n",
    "AIR_TIME The time duration between wheels_off and wheels_on time\n",
    "\n",
    "DISTANCE Distance between two airports\n",
    "\n",
    "WHEELS_ON The time point that the aircraft's wheels touch on the ground\n",
    "\n",
    "TAXI_IN The time duration elapsed between wheels-on and gate arrival at the destination airport\n",
    "\n",
    "SCHEDULED_ARRIVAL Planned arrival time\n",
    "\n",
    "ARRIVAL_TIME:  WHEELS_ON + TAXI_IN\n",
    "\n",
    "ARRIVAL_DELAY: ARRIVAL_TIME - SCHEDULED_ARRIVAL\n",
    "\n",
    "DIVERTED Aircraft landed on airport that out of schedule\n",
    "\n",
    "CANCELLED Flight Cancelled (1 = cancelled)\n",
    "\n",
    "CANCELLATION_REASON Reason for Cancellation of flight: A - Airline/Carrier; B - Weather; C - National Air System; D - Security\n",
    "\n",
    "AIR_SYSTEM_DELAY Delay caused by air system\n",
    "\n",
    "SECURITY_DELAY Delay caused by security\n",
    "\n",
    "AIRLINE_DELAY Delay caused by the airline\n",
    "\n",
    "LATE_AIRCRAFT_DELAY Delay caused by aircraft\n",
    "\n",
    "WEATHER_DELAY Delay caused by weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file as dataframe \n",
    "# import pyspark modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, Bucketizer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"app\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config('spark.cores.max', '2') \\\n",
    "    .config(\"spark.driver.memory\",'4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of APT edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(\"/home/jovyan/FlightDelay/flights.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read into rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_rdd = sc.textFile(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").option(\"inferschema\",\"true\").load(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_df = df.sample(False, .02, 898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delay_df.where(new_df.CANCELLED == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|  DEPARTURE_DELAY|    ARRIVAL_DELAY|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|           104039|           103692|\n",
      "|   mean|9.302674958429051|4.378515218146048|\n",
      "| stddev|37.15192259780152|39.17235225813305|\n",
      "|    min|              -37|              -69|\n",
      "|    max|             1457|             1460|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_df.describe('DEPARTURE_DELAY', 'ARRIVAL_DELAY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears there are no duplicated entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+-------------------+---------------+---------------------+--------------------+----------------------+---------------------------+---------------------------+----------------------+-----------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------------+--------------------+--------------------+-------------------------+--------------------+---------------------+----------------+-----------------+---------------------------+------------------------+----------------------+---------------------+---------------------------+---------------------+\n",
      "|YEAR_missing|MONTH_missing|DAY_missing|DAY_OF_WEEK_missing|AIRLINE_missing|FLIGHT_NUMBER_missing| TAIL_NUMBER_missing|ORIGIN_AIRPORT_missing|DESTINATION_AIRPORT_missing|SCHEDULED_DEPARTURE_missing|DEPARTURE_TIME_missing|DEPARTURE_DELAY_missing|    TAXI_OUT_missing|  WHEELS_OFF_missing|SCHEDULED_TIME_missing|ELAPSED_TIME_missing|    AIR_TIME_missing|DISTANCE_missing|   WHEELS_ON_missing|     TAXI_IN_missing|SCHEDULED_ARRIVAL_missing|ARRIVAL_TIME_missing|ARRIVAL_DELAY_missing|DIVERTED_missing|CANCELLED_missing|CANCELLATION_REASON_missing|AIR_SYSTEM_DELAY_missing|SECURITY_DELAY_missing|AIRLINE_DELAY_missing|LATE_AIRCRAFT_DELAY_missing|WEATHER_DELAY_missing|\n",
      "+------------+-------------+-----------+-------------------+---------------+---------------------+--------------------+----------------------+---------------------------+---------------------------+----------------------+-----------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------------+--------------------+--------------------+-------------------------+--------------------+---------------------+----------------+-----------------+---------------------------+------------------------+----------------------+---------------------+---------------------------+---------------------+\n",
      "|         0.0|          0.0|        0.0|                0.0|            0.0|                  0.0|0.002452465722293...|                   0.0|                        0.0|                        0.0|  0.014856829028103924|   0.014856829028103924|0.015339746988864533|0.015339746988864533|                   0.0|0.018142564957200236|0.018142564957200236|             0.0|0.015964699643966407|0.015964699643966407|                      0.0|0.015964699643966407| 0.018142564957200236|             0.0|              0.0|         0.9844614044390576|       0.817210817362321|     0.817210817362321|    0.817210817362321|          0.817210817362321|    0.817210817362321|\n",
      "+------------+-------------+-----------+-------------------+---------------+---------------------+--------------------+----------------------+---------------------------+---------------------------+----------------------+-----------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------------+--------------------+--------------------+-------------------------+--------------------+---------------------+----------------+-----------------+---------------------------+------------------------+----------------------+---------------------+---------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculates percent of missing values in ecah column! \n",
    "missing = delay_df.agg(*[\n",
    "    (1-F.count(c) / F.count('*')).alias(c + '_missing')\n",
    "    for c in delay_df.columns\n",
    "]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last 6 coluns appear to have very large percentage of missing values: \n",
    "\n",
    "CANCELLATION_REASON_missing, AIR_SYSTEM_DELAY_missing, SECURITY_DELAY_missing, \n",
    "AIRLINE_DELAY_missing, LATE_AIRCRAFT_DELAY_missing, WEATHER_DELAY_missing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should I drop all of these columns? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns selected have almost 90% na, so I'm dropping them from the dataet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['WEATHER_DELAY', 'SECURITY_DELAY', 'AIR_SYSTEM_DELAY','AIRLINE_DELAY', \n",
    "                'LATE_AIRCRAFT_DELAY', 'CANCELLATION_REASON', 'WHEELS_ON', 'WHEELS_OFF', \n",
    "                'TAXI_IN', 'TAXI_OUT', 'AIR_TIME', 'TAIL_NUMBER'] \n",
    "\n",
    "delay_df = delay_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records where atleast 3 columns have NULL values \n",
    "\n",
    "delay_df = delay_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't change the count of the DF, so no rows with atleast 3 missing columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT', 'DIVERTED', \n",
    "                   'CANCELLED', 'DESTINATION_AIRPORT', 'AIRLINE']\n",
    "\n",
    "\n",
    "df_impute = delay_df.drop(*drop_cols)\n",
    "means = df_impute.agg(*[F.mean(c).alias(c) \\\n",
    "                                for c in df_impute.columns]) \\\n",
    "                                .toPandas().to_dict('records')[0]\n",
    "\n",
    "df_impute_mode = delay_df.select('YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT', \n",
    "                                 'DIVERTED','CANCELLED', 'DESTINATION_AIRPORT', 'AIRLINE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = []\n",
    "for c in df_impute_mode.columns:\n",
    "    df=df_impute_mode.groupBy(c).count()\n",
    "    mode = df.orderBy(df['count'].desc()).collect()[0][0]\n",
    "    modes.append((c,mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105608"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn list of tuples to dictionary \n",
    "modes = dict(modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dictionaries\n",
    "def Merge(dict1, dict2): \n",
    "    res = {**dict1, **dict2} \n",
    "    return res \n",
    "\n",
    "imputed_vals = Merge(means,modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to impute values for these columns since I dont know how imputing integer values such as day of the week or year will affect the data. We can ask the professor about this! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill na values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delay_df = delay_df.fillna(imputed_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+---------------+---------+--------------+------------+-------------+\n",
      "|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|CANCELLED|SCHEDULED_TIME|ARRIVAL_TIME|ARRIVAL_DELAY|\n",
      "+-------------------+--------------+---------------+---------+--------------+------------+-------------+\n",
      "|                200|          1336|              9|        1|           120|        1476|            4|\n",
      "|                220|           209|            -11|        0|           224|         728|          -36|\n",
      "|                545|           603|             18|        0|           124|         847|           -2|\n",
      "|                545|          1336|              9|        1|            66|        1476|            4|\n",
      "|                550|           605|             15|        0|           140|         831|           21|\n",
      "+-------------------+--------------+---------------+---------+--------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_df.select('SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', \n",
    "                'DEPARTURE_DELAY','CANCELLED', \n",
    "                'SCHEDULED_TIME', 'ARRIVAL_TIME', 'ARRIVAL_DELAY').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#delay_df.select('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate values used for outlier filtering\n",
    "\n",
    "df_for_outlier_calc = delay_df.select('DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ELAPSED_TIME', 'DISTANCE')\n",
    "\n",
    "for c in df_for_outlier_calc.columns:\n",
    "    mean_val = delay_df.agg({c: 'mean'}).collect()[0][0]\n",
    "    stddev_val = delay_df.agg({c: 'stddev'}).collect()[0][0]\n",
    "\n",
    "    # Create three standard deviation (μ ± 3.3σ) lower and upper bounds for data\n",
    "    # Use 3.3 since our data is not normally distrubuted and we should expand bounds to deal with this \n",
    "    low_bound = mean_val - (3.3 * stddev_val)\n",
    "    hi_bound = mean_val + (3.3 * stddev_val)\n",
    "\n",
    "    # Filter the data to fit between the lower and upper bounds\n",
    "    delay_df = delay_df.where((delay_df[c] < hi_bound) & (delay_df[c] > low_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delay_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should I get rid of these outliers or impute them? It seems like a lot of data to impute or get rid of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider creating a variable that uses a ratio of elasped time to distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoder \n",
    "\n",
    "We want to use OneHotEncoder on the string type variables: \n",
    "'AIRLINE', 'DESTINATION_AIRPORT' ,' ORGIN_AIRPORT' \n",
    "to represent them in a numerical form.\n",
    "\n",
    "Maps a column of label indices to a column of binary vectors, with at most a single one-value. This is the same as dummy coding. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.\n",
    "\n",
    "\n",
    "An intermediate step is to use StringIndexer.\n",
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply OneHotEncoder to AIRLINE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"AIRLINE\", outputCol=\"AIRLINE_Index\")\n",
    "model = stringIndexer.fit(delay_df)\n",
    "indexed = model.transform(delay_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"AIRLINE_Index\", outputCol=\"AIRLINE_Vec\")\n",
    "encoded = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply OneHotEncoder to Orgin_AIRPORT: \n",
    "\n",
    "Is there a way ro encode origin_airport and destination_airport \n",
    "together so the same airports have the same encoder inboth columns? \n",
    "\n",
    "How do we use OneHotEncoder column?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer2 = StringIndexer(inputCol=\"ORIGIN_AIRPORT\", outputCol=\"ORIGIN_AIRPORT_Index\")\n",
    "model2 = stringIndexer2.fit(encoded)\n",
    "indexed2 = model2.transform(encoded)\n",
    "\n",
    "encoder2 = OneHotEncoder(inputCol=\"ORIGIN_AIRPORT_Index\", outputCol=\"ORIGIN_AIRPORT_Vec\")\n",
    "encoded2 = encoder2.transform(indexed2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------------+-----------------------+\n",
      "|DESTINATION_AIRPORT|DESTINATION_AIRPORT_Index|DESTINATION_AIRPORT_Vec|\n",
      "+-------------------+-------------------------+-----------------------+\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                CLT|                     14.0|       (585,[14],[1.0])|\n",
      "|                ATL|                      0.0|        (585,[0],[1.0])|\n",
      "|                LAX|                      3.0|        (585,[3],[1.0])|\n",
      "|                IAH|                      5.0|        (585,[5],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                FLL|                     22.0|       (585,[22],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                MCO|                     10.0|       (585,[10],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                PHL|                     24.0|       (585,[24],[1.0])|\n",
      "|                ATL|                      0.0|        (585,[0],[1.0])|\n",
      "|                LAS|                      7.0|        (585,[7],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                DFW|                      2.0|        (585,[2],[1.0])|\n",
      "|                SEA|                     11.0|       (585,[11],[1.0])|\n",
      "|                ORD|                      1.0|        (585,[1],[1.0])|\n",
      "|                DEN|                      4.0|        (585,[4],[1.0])|\n",
      "+-------------------+-------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "\n",
    "stringIndexer3 = StringIndexer(inputCol=\"DESTINATION_AIRPORT\", outputCol=\"DESTINATION_AIRPORT_Index\")\n",
    "model3 = stringIndexer3.fit(encoded2)\n",
    "indexed3 = model3.transform(encoded2)\n",
    "\n",
    "encoder3 = OneHotEncoder(inputCol=\"DESTINATION_AIRPORT_Index\", outputCol=\"DESTINATION_AIRPORT_Vec\")\n",
    "encoded3 = encoder3.transform(indexed3)\n",
    "encoded3.select('DESTINATION_AIRPORT','DESTINATION_AIRPORT_Index', \"DESTINATION_AIRPORT_Vec\").show()\n",
    "#encoded3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecesary Columns from encoded3 dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, DISTANCE: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, AIRLINE_Vec: vector, ORIGIN_AIRPORT_Vec: vector, DESTINATION_AIRPORT_Vec: vector]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols_to_drop = ['AIRLINE_Index', 'AIRLINE', 'ORIGIN_AIRPORT_Index', \n",
    "                                   'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT_Index', 'DESTINATION_AIRPORT', 'FLIGHT_NUMBER']\n",
    "\n",
    "final_encoded = encoded3.drop(*new_cols_to_drop)\n",
    "\n",
    "final_encoded.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_splits = [0, 300, 600, 900, 1200, 1500, 1800, 2100, 2400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the job, quickly too, but not very elegantly. Look into how to bucketize groups of columns\n",
    "\n",
    "deptime_bucketizer = Bucketizer() \\\n",
    "  .setInputCol(\"DEPARTURE_TIME\") \\\n",
    "  .setOutputCol(\"B_DEPARTURE_TIME\") \\\n",
    "  .setSplits(delay_splits)\n",
    "\n",
    "scheddep_bucketizer = Bucketizer() \\\n",
    "  .setInputCol(\"SCHEDULED_DEPARTURE\") \\\n",
    "  .setOutputCol(\"B_SCHEDULED_DEPARTURE\") \\\n",
    "  .setSplits(delay_splits)\n",
    "\n",
    "arrtime_bucketizer = Bucketizer() \\\n",
    "  .setInputCol(\"ARRIVAL_TIME\") \\\n",
    "  .setOutputCol(\"B_ARRIVAL_TIME\") \\\n",
    "  .setSplits(delay_splits)\n",
    "\n",
    "schedarr_bucketizer = Bucketizer() \\\n",
    "  .setInputCol(\"SCHEDULED_ARRIVAL\") \\\n",
    "  .setOutputCol(\"B_SCHEDULED_ARRIVAL\") \\\n",
    "  .setSplits(delay_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform original data into its bucket index.\n",
    "final_df = deptime_bucketizer\\\n",
    "               .transform(scheddep_bucketizer\\\n",
    "               .transform(arrtime_bucketizer\\\n",
    "               .transform(schedarr_bucketizer\\\n",
    "               .transform(delay_df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write clean_data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = final_df.toPandas()\n",
    "new_df.to_csv('clean_data_no_hot', sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
